import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout
df = pd.read_csv('ecommerce_datsaset.csv',usecols=['Text', 'label'])
df['Text'] = df['Text'].astype(str)
label_counts = df['label'].value_counts()
valid_labels = label_counts[label_counts > 1].index
df = df[df['label'].isin(valid_labels)]
label_encoder = LabelEncoder()
df['label_encoded'] = label_encoder.fit_transform(df['label'])
tokenizer = Tokenizer(oov_token='<OOV>')
tokenizer.fit_on_texts(df['Text'])
sequences = tokenizer.texts_to_sequences(df['Text'])
word_index = tokenizer.word_index
max_length = 100
X = pad_sequences(sequences, maxlen=max_length, padding='post')
y = df['label_encoded'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
# Model
vocab_size = len(word_index) + 1
embedding_dim = 64
model = Sequential([
    Embedding(vocab_size, embedding_dim, input_length=max_length),
    Bidirectional(LSTM(64, return_sequences=False)),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dense(len(np.unique(y)), activation='softmax')
])
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)
 
# Sample predictions
sample_texts = [
    "Bluetooth speaker with long battery life",
    "Casual summer dress for women",
    "Hardcover fiction novel by famous author",
    "Comfortable cotton bed sheets",
    "Smartwatch with fitness tracking",
    "Men's running shoes with arch support",
    "Designer handbag made from leather",
    "Children’s illustrated story book",
    "LED monitor for gaming setup",
    "Kitchen blender with multiple speed settings"
]
sample_seq = tokenizer.texts_to_sequences(sample_texts)
sample_pad = pad_sequences(sample_seq, maxlen=max_length, padding='post')
predictions = model.predict(sample_pad)
for i, text in enumerate(sample_texts):
    pred_class = label_encoder.inverse_transform([np.argmax(predictions[i])])[0]
    print(f"Text: {text} → Predicted Class: {pred_class}")
 
# Evaluation
y_pred = np.argmax(model.predict(X_test), axis=1)
print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))
 
# Accuracy & loss plot
plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.legend()
plt.title('Accuracy over epochs')
plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend()
plt.title('Loss over epochs')
plt.show()
# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
